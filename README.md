# aws-cloud-practitioner
## concepts - ELI5:

### S3 bucket & objects
#### acl vs public bucket policy vs access point policies
##### ACLs: ACLs are a legacy method of controlling access to S3 resources. ACLs are attached to individual S3 objects or buckets and are used to grant or deny permissions to specific users or groups. ACLs can be used to define fine-grained access control for your S3 resources, but can be difficult to manage at scale. Bucket policies: are JSON-based documents that are attached to an S3 bucket and are used to control access to the bucket and its objects. Bucket policies can be used to grant or deny permissions to specific users, groups, or AWS accounts, and can be used to define fine-grained access control for your S3 resources. Bucket policies are more flexible and easier to manage than ACLs, and can be used to define more complex access control scenarios. Access point policies: Access point policies are JSON-based documents that are attached to an S3 access point and are used to control access to the access point and its objects. Access point policies can be used to grant or deny permissions to specific users, groups, or AWS accounts, and can be used to define fine-grained access control for your S3 resources. Access point policies are similar to bucket policies, but are attached to an access point instead of a bucket. The main difference between bucket policies and access point policies is the scope of the policy. Bucket policies apply to the entire bucket and all of its objects, while access point policies apply only to the access point and its objects. This means that access point policies can be used to define more granular access control for specific access points, while bucket policies provide broader access control for entire buckets.
#### bucket policy vs user policy vs arn
##### In AWS S3, bucket policies and user policies are two types of policies that are used to control access to S3 resources. ARN (Amazon Resource Name) is a unique identifier for an AWS resource, such as an S3 bucket or object. ARNs are used to specify the resource to which the policy applies. Bucket policies are attached to an S3 bucket and are used to control access to the bucket and its objects. user policies on the other hand are attached to an IAM user or group and are used to control access to specific S3 resources, by going to IAM dashboard in the aws search console you can create an IAM USER OR GROUP and attach one or more policies to the user or group. User policies are attached to individual IAM users, while group policies are attached to IAM groups, depends on what you defined and configured at IAM DASHBOARD.
#### object writer
##### object writer is a user or application that has permission to write objects to an S3 bucket. An object writer can upload new objects to an S3 bucket, modify existing objects, or delete objects from the bucket. you edit the premissio under the bucket premission tab then choose bucket policy -> edit and enter a json format policy to grant premission by specifcing the user arn id and specify what is allowed to edit, add, remove etc. Object write permission can be granted using S3 bucket policies or IAM policies, allowing you to control access to your S3 resources and improve security.
#### sse-s3 encryption
##### (Server-Side Encryption with S3-Managed Keys) is a feature that provides encryption of S3 objects at rest. SSE-S3 automatically encrypts objects when they are stored in S3, and decrypts them when they are retrieved. When SSE-S3 is enabled, S3 uses 256-bit Advanced Encryption Standard (AES-256) encryption to encrypt objects. The encryption keys are managed by S3, and are stored separately from the objects themselves. This provides an additional layer of security for your S3 objects, as it ensures that the encryption keys are not accessible to unauthorized users. With SSE-S3 enabled, all objects that are uploaded to the bucket will be automatically encrypted with AES-256 encryption. When objects are retrieved, S3 automatically decrypts them using the encryption keys that are managed by S3. It's worth noting that there are other methods of encrypting S3 objects, such as SSE-KMS (Server-Side Encryption with AWS KMS-Managed Keys) and SSE-C (Server-Side Encryption with Customer-Provided Keys). These methods provide more granular control over encryption keys and can be used in conjunction with SSE-S3 to provide comprehensive encryption of S3 objects. the encryption is used at "REST" only which means it protects against unauthorized access or data breaches to the PHYSICAL storage device, such as a hard drive or solid-state drive ONLY.
#### bucket key - enabled
##### bucket key enabled is a feature that allows you to use AWS KMS (Key Management Service) to manage the encryption keys for your S3 objects. When bucket key enabled is enabled, S3 generates a unique data key for each object that is stored in the bucket, and then encrypts the data key using a KMS customer master key (CMK) that the aws user provided when choosing to enable this option. Bucket key enabled can be used in conjunction with SSE-S3 (Server-Side Encryption with S3-Managed Keys) to provide additional encryption of S3 objects. When both bucket key enabled and SSE-S3 are enabled, S3 uses a unique data key for each object that is stored in the bucket, and encrypts the data key using a KMS customer master key (CMK). The data itself is also encrypted using SSE-S3.
#### s3 static website hosting
##### S3, static website hosting is a feature that allows you to host static websites directly from an S3 bucket. Static websites are websites that consist of HTML, CSS, JavaScript, and other static files, and do not require server-side processing or dynamic content. With static website hosting enabled, your S3 bucket will be configured to serve your static website files as a website. You can access your website using the endpoint URL that is provided in the "Static website hosting" section of the bucket properties.  If your website requires server-side processing or dynamic content, you may need to use a different hosting solution, such as Amazon EC2 or AWS Elastic Beanstalk. Overall, static website hosting in AWS S3 is a simple and cost-effective way to host static websites directly from an S3 bucket. It's easy to set up and can be used for a variety of use cases, such as personal websites, blogs, and small business websites. S3 simply serves the static website files directly from the bucket, using the bucket like a webserver by indicating the default index.html and error.html that the user provided as the entry point for the url that aws provides.
#### s3 virtual hosted style url vs a path style url
##### In AWS S3, there are two types of URLs that can be used to access S3 objects: virtual hosted style URLs and path style URLs. Virtual hosted style URLs are URLs that use the BUCKET NAME as part of the domain name in the URL. For example, a virtual hosted style URL for an S3 object might look like this: http://mybucket.s3.amazonaws.com/myobject - Path style URLs, on the other hand, are URLs that use the bucket name as part of the PATH in the URL. For example, a path style URL for the same S3 object might look like this: http://s3.amazonaws.com/mybucket/myobject - Virtual hosted style URLs are generally preferred over path style URLs, as they provide better performance and scalability. Virtual hosted style URLs allow S3 to distribute requests across multiple servers, which can improve performance and reduce latency. Path style URLs, on the other hand, require S3 to route all requests through a single server, which can limit performance and scalability.

### ec2 
#### ami
##### amazon-machine-image is required to launch an instance in AWS. it convenient way to launch instances with pre-configured software and settings, This allows you to launch instances with the same configuration as the original AMI, including the same hardware resources and network settings. You can choose from a variety of pre-configured AMIs provided by AWS or the AWS Marketplace, or create your own custom AMI to meet your specific needs.
#### instance type (server type)
##### instance type is a specification that defines the hardware resources, such as CPU, memory, and storage, that are available for an EC2 instance. Each instance type is optimized for specific use cases and workloads, and provides a different balance of compute, memory, and storage resources.
#### storage gib & root volume & ebs & gp3 iops
##### Storage in EC2 is measured in gibibytes (GiB), which is a binary unit of measurement that is equivalent to 1,073,741,824 bytes. The root volume is the primary storage device that is attached to an EC2 instance and is used to store the operating system and other system files. EBS (Elastic Block Store) volumes are used to store data and can be attached and detached from instances as needed. GP3 (general purpose ssd) are EBS volumes that provide a flexible and cost-effective way to provision storage with customizable IOPS (Input/Output Operations Per Second) and throughput.
#### Compute vCPUs 
##### Compute resources in EC2 are measured in vCPUs (virtual CPUs), which are allocated to instances based on the instance type. Each instance type provides a different balance of compute, memory, and storage resources, and is optimized for specific use cases and workloads.
#### Snapshots (frequency and size changed per snapshot)
##### EBS snapshots are point-in-time copies of EBS volumes that can be used to back up and restore data. The frequency and size of EBS snapshots can be customized based on your specific backup and recovery requirements.
#### Data transfer (inbound and outbound data transfer and intraregion data transfer)
##### Inbound and outbound data transfer in EC2 is measured in gigabytes (GB) and is charged based on the amount of data transferred. Intra-region data transfer, which is data transfer between EC2 instances in the same AWS region, is typically free or charged at a lower rate than inter-region data transfer.
#### key pair
##### 
#### firewall security groups rules
##### 
#### user data (optinal meta-data script)
#####
#### instance connect vs session manager vs ssh vs serial console
##### 
#### public ipv4 & private ipv4 & public ipv4 dns 
##### 
#### nacl (network access control list)
##### is the firewall at the subnet level / boundary, it comes with a default one when creating every subnet which allows BY DEFAULT all inbound and outbound traffic but you can create a custom nacl and attach/assoicate it with your subnet. you can attach multiple subnets to NACL, but only 1 nacl can be attached to each subnet. nacl are stateless and required specific rules for inbound and outbound traffic
#### security groups
##### security groups are a firewall on a INSTANCE level (website, db, virtual machine etc). by default the security group that attached to each instance denys all inbound and ALLOWS all outbound traffic. security groups are statefull and allows responses to inbound traffic automatically without the need to configure outbound traffic and same for vice versa. unlike NACL where if one rule allows the traffic it will pass (sorted by order from 100 which is the strongest, 200 weaker then 300 etc), in security groups all the rules are evulated before deciding whether to allow traffic in / out or not, traffic can be restricted by ip type (ssh,), protocol (tcp, http), port range (80, 443), traffic soucce (cidr-classless inter domain routes) example of using security groups: webserver instance allows inbound traffic from internet 0.0.0.0/0 for https port 443. the application instance allows http port 80 from the webserver instance only. the database instance allows inbound traffic on tcp port 3306 from application instance ip only. and remmeber secuirty groups are stateful so no need to configure outbound rules as it's automatically sent back. this creates very safety network as even if someone hacks the webserver and wants to steal data they still can't access the database directly.

### VPC
#### VPC
##### 
#### subnet
#####
#### route table
#####
#### internet gateway - igw
#####
#### nat gateway - nat (network address translation)
##### nat gateway translate the ip address of a private subnet into the nat gateway ip address in order to commuincate with the INTERNET from the PRIVATE subnet. when a request is sent from a private subnet or the internet that looks to reach the private subnet the nat gateway is the ONLY gateway the translate the request to / from the private ip address of the subnet. To setup a nat gateway in the public subnet, you must also specificy an elastic ip while creating the NAT gateway in a public subnet. after creating the NAT gateway you must add to the route table of the PRIVATE subnet the 0.0.0.0 IP destination with the target as nat-gwy-id** (ipv4 only) for ipv6 use egress-only-igw which allows connection from private subnets to the internet in your vpc which also blocks direct communication between the internet to the private subnet and add to the private subnet route table the ::/0 - eigw-id** , egress only is stateful so it remmbers who it sent the content to and allows automatically to get back a response from the destination if inbound premission isn't allowed to the private subnet. To allows access to the internet from / to the private subnet you MUST also configure a route table inside the private subnet to allow specific types of traffic and ips to & from the subnet. 
#### vpc peering
##### connecting different vpcs to each other (also from different aws accounts), so data-information can be transferred between varies vpc's instaces. you add to the route table of the requester vpc: 1. destination=the cidr block of the reciever vpc. 2. target=pcx-** (peering connection). only a direct connection between peers exists (if both VPC A & C connects to VPC B via peering, A and C CAN NOT communicate with each other) aka tranitive or edge to edge routing 
#### ROOT USER VS IAM USER
##### In AWS, the root user is the initial user account that is created when you first create an AWS account. The root user has full access to all AWS services and resources in the account, and can perform any action on those resources. As such, it is recommended that you do not use the root user for day-to-day activities, but instead create and use IAM (Identity and Access Management) users. IAM users are separate user accounts that you can create and manage within your AWS ROOT account. IAM users have specific permissions that you can control and manage, allowing you to grant or restrict access to AWS services and resources as needed. IAM users can be assigned to groups, which can simplify the process of managing permissions for multiple users. this provides improved security (limit scope to your resources), accountability (track and audit actions taken by individual users), management (can control access of all users to every resource from one account)